---
title: "ECONOMETRIC AND STATISTICAL LEARNING MODELS FOR CANCER DETECTION WITH DATA FROM COMPUTATIONAL GENOMICS"
output: html_notebook
---

# Author: Pablo Campos Viana

## Chapter 3 - Models and results

We begin by loading the required packages.
```{r}
library(TCGA2STAT)
library(dplyr)
library(parallel)
library(doParallel)
library(caret)
library(kernlab)
library(Rtsne)
library(glmnet)
library(ggfortify)
```


We download and load the data from the TCGA portal using the *getTCGA* function from the **TCGA2STAT** package. We assign labels to each type of cancer and we merge both of them in a single dataframe. We can count how many cases of each cancer there are in the data.
```{r}
cesc <- getTCGA(disease="CESC", data.type="RNASeq2", clinical = F)
ucec <- getTCGA(disease="UCEC", data.type="RNASeq2", clinical = F)

df.cesc <- as.data.frame(t(cesc$dat)) %>% mutate(label = "CESC") 
df.ucec <- as.data.frame(t(ucec$dat)) %>% mutate(label = "UCEC") 

df <- rbind(df.cesc, df.ucec)
rm(df.cesc,df.ucec)

table(df$label)
```

We can use all the cores from our computer to perform the grid search and cross validation in parallel.
```{r}
registerDoParallel(cores=detectCores())
getDoParWorkers()
```

We split the data into train and test datasets. We assign 75% of the data to train dataset.
```{r}
set.seed(123)
inTrain <- createDataPartition(y = df$label, p = .75, list = FALSE)
train <- df[inTrain,]
test <- df[-inTrain,]
```
We can see how many cases lies in each of these partitions.
```{r}
print(train %>% nrow)
print(test %>% nrow)
```

## 3.2 - Support vector machines

We define a function to perform the training process using the *train* function from the **caret** package. Inside we set a seed in order to keep reproducibility. Implicitly, the *ksvm* function from the **kernlab** package is used to fit the SVM models.

```{r}
fitting <- function(method, tune.grid, varsmodel) {
  set.seed(108783)
  fit <- train(x=(train %>% select(one_of(varsmodel))), y=train$label,
                method = method, tuneGrid = tune.grid,
                trControl = train.control, scaled = FALSE, preProcess = c("center","scale"))
  
  preds <- predict(fit, newdata =  (test %>% select(one_of(varsmodel))))
  cm <- confusionMatrix(preds,test$label)
  print(fit)
  print(cm)
  return(fit)
  }
```

### Feature selection
We can define a function to select some top variables based on higher standar deviations, filter highly correlated variables and then perform the *RFE* routine.
```{r}
profiling <- function(n_vars,tune.grid,method){
  sds <- apply((train %>% select(-label)), 2, sd) %>% sort(decreasing=T)
  topvars <- sds %>% head(n_vars) %>% names

  to_remove <- train %>% 
  select(one_of(topvars)) %>% 
  cor(.) %>% 
  findCorrelation(.,cutoff = .70, verbose=F, names=T)

  topvars <- setdiff(topvars,to_remove)

  ctrl <- rfeControl(functions=caretFuncs, 
                   method = "cv", number = 3,
                   returnResamp="final", verbose = TRUE)

  subsets <- c(1:5, seq(10, 100, by = 10))
  
  set.seed(108783)
  profile <- rfe(x = train %>% select(one_of(topvars)), y = factor(train$label), 
                 sizes=subsets,
                 rfeControl=ctrl,preProcess = c("center","scale"),
                 method=method, tuneGrid = tune.grid)
  return(profile)
}
```

#### Linear Kernel
```{r}
tune.grid <- expand.grid(list(C=c(0.1)))
lin.profile <- profiling(500,tune.grid,"svmLinear")
print(lin.profile)
```

#### Non-linear Kernel
```{r}
tune.grid <- expand.grid(list(sigma=c(0.1),C=c(1)))
rf.profile <- profiling(500,tune.grid,"svmRadial")
print(rf.profile)
```

### Parameter selection

We set the cross validation as our resampling method in the training process and its number of folds.
```{r}
train.control <- trainControl(method='cv', number=3)
```

#### Linear Kernel

And now we can use our previously defined function. First, we try with the linear Kernel.

```{r}
tune.grid <- expand.grid(list(C=c(0.1,0.25,0.5,1,10,100)))
svmFitLin <- fitting("svmLinear",tune.grid,predictors(lin.profile))
```

#### Non-linear Kernel

And now we try with the Radial Kernel.
```{r}
tune.grid <- expand.grid(list(sigma=c(0.1,0.25,0.5,1,10,100),
                              C=c(0.1,0.25,0.5,1,10,100)))
svmFitRad <- fitting("svmRadial",tune.grid,predictors(rf.profile))
```

Also we can visualize the most relevant features based on variable importance.
```{r}
plot(varImp(svmFitRad, scale = FALSE))
```

### Dimensionality reduction: t-SNE

Now we perform dimensionality reduction through the t-SNE algorithm. We use the **Rtsne** package and set the dimension of the embedded space to be 2.

We need to modify slightly our fitting function in order to be performed over our embedded space.
```{r}
fitting_embedd <- function(method, tune.grid) {
  set.seed(108783)
  fit <- train(x=(embedd[inTrain,]), y=train$label,
                method = method, tuneGrid = tune.grid,
                trControl = train.control, scaled = FALSE, preProcess = c("center","scale"))
  
  preds <- predict(fit, newdata =  embedd[-inTrain,])
  cm <- confusionMatrix(preds,test$label)
  print(fit)
  print(cm)
  acc_labels <- if_else(preds != test$label, "Incorrect", "Correct")
  return(list(fit,acc_labels))
  }
```

Now we run t-SNE algorithm and get the low dimensional representation of our data.
```{r}
tsne <- Rtsne(as.matrix(df %>% select(-label)), dims = 2, perplexity=30, verbose=TRUE, max_iter = 1000)
embedd <- data.frame(tsne$Y)
```

Now that we have performed the embedding, we can call our previously defined function. As before, we try with both the linear Kernel and the non-linear Kernel.

#### Linear Kernel
```{r}
tune.grid <- expand.grid(list(C=c(0.1,0.25,0.5,1,10,100)))
output <- fitting_embedd("svmLinear",tune.grid)
svmFitLin_tsne <- output[[1]]
acc_labels_linear <- output[[2]]
rm(output)
```

We can visualize the classification regions learned by the linear model in the embedded space.

```{r}
dat.grid <- expand.grid(X1=seq(-20,20,0.1), X2=seq(-15,15,0.1))
dat.grid$preds_linear <- predict(svmFitLin_tsne, newdata = dat.grid[,1:2])

plot_linear <- ggplot(dat.grid, aes(x=X1, y=X2))+geom_point(size=1, alpha=0.2, aes(colour=preds_linear)) +
  geom_point(data=embedd[-inTrain,], aes(colour=factor(test$label),size = factor(acc_labels_linear))) +
  xlim(-20,20) + ylim(-15,15) + 
  theme(legend.title=element_blank()) +
  xlab("1st dimension") + ylab("2nd dimension")

```

```{r, fig.width = 12, fig.height = 12}
#plot_linear
```

#### Non-linear Kernel
```{r}
tune.grid <- expand.grid(list(sigma=c(0.1,0.25,0.5,1,10,100),
                              C=c(0.1,0.25,0.5,1,10,100)))
output <- fitting_embedd("svmRadial",tune.grid)
svmFitRad_tsne <- output[[1]]
acc_labels_radial <- output[[2]]
rm(output)
```


We can visualize the classification regions learned by the non-linear model in the embedded space.

```{r}
#dat.grid <- expand.grid(X1=seq(-20,20,0.1), X2=seq(-15,15,0.1))
dat.grid$preds_radial <- predict(svmFitRad_tsne, newdata = dat.grid[,1:2])

plot_radial <- ggplot(dat.grid, aes(x=X1, y=X2))+geom_point(size=1, alpha=0.2, aes(colour=preds_radial)) +
  geom_point(data=embedd[-inTrain,], aes(colour=factor(test$label),size = factor(acc_labels_radial))) +
  xlim(-20,20) + ylim(-15,15) + 
  theme(legend.title=element_blank()) +
  xlab("1st dimension") + ylab("2nd dimension")

```

```{r, fig.width = 12, fig.height = 12}
#plot_radial
```

## 3.3 - Logistic regression

### Feature selection: regularized model

Now we perform L1 regularized logistic regression with cross validation using the *cv.glmnet* function from **glmnet** package in order to select the best predictor variables so we can use them later in a standard logistic regression model. We select the variables based on the largest value of lambda such that error is within one standard error of the minimum.

```{r}
set.seed(108783)
lasso <- cv.glmnet(x=as.matrix(train %>% select(-label)), 
                   y = as.numeric(if_else(train$label == "CESC",1,0)),
                   family="binomial",
                   nfolds = 3, type.measure = "class")

lassovars <- colnames(train)[which(coef(lasso, s = "lambda.1se") != 0)]
print(lassovars)
```

### Regularization parameter selection
We can visualize the feature selection process as a function of the regularization parameter.
```{r}
autoplot(lasso, colour = 'red') + xlab(expression(log(lambda))) + ylab("Misclassification error")
```

### Non-regularized model

Once we have chosen the best predictor variables, we can use them to fit a standard logistic regression model. We define a function to perform some preprocessing, fit the model and predict classes in the test dataset. We use a simple cutoff point of 0.5 to assign the classes based on the probabilities.

```{r}
fitting_glm<-function(varsmodel) {
  pp <- preProcess(train %>% select(one_of(varsmodel)), method=c("center","scale"))
  xaux <- predict(pp,train %>% select(one_of(varsmodel)))
  yaux <- as.numeric(if_else(train$label == "CESC",1,0))
  dfaux <- cbind(xaux,yaux)
  rm(xaux,yaux)
  
  model <- glm(as.numeric(yaux) ~ ., data = dfaux, family=binomial)
  
  xauxt <- predict(pp,test %>% select(one_of(varsmodel)))
  yauxt <- as.numeric(if_else(test$label == "CESC",1,0))
  dfauxt <- cbind(xauxt,yauxt)
  rm(xauxt,yauxt)
  preds_model <- predict.glm(model,newdata=dfauxt,type="response") 
  preds_model <- if_else(preds_model >= 0.5, "CESC", "UCEC")
  
  cm <- confusionMatrix(preds_model,test$label)
  print(summary(model))
  print(cm)
  return(model)
}
```

Now we apply our previously defined function and see how it performs.
```{r}
full_model <- fitting_glm(lassovars)
```

We observe that not all variables are statistically significant according to this specification. We can keep the variables that in fact are significant.
```{r}
sub_lassovars <- names(which((summary(full_model)$coeff[-1,4] < 0.05)))
print(sub_lassovars)
```

And finally we apply our previously defined function over this subset of variables to get a parsimonious model and see how it performs.
```{r}
reduced_model <- fitting_glm(sub_lassovars)
```

Additionally, if we would like to give some interpretation to the variables used in the SVM model with the non-linear Kernel, we can see how a logistic regression model perform using this set of variables. 

```{r}
topvars_model <- fitting_glm(predictors(rf.profile))
```

And we can see which one of these variables are in fact significant according to this specification. Also, we can give some interpretation based on the sign of the parameters.

#### Session Information
```{r}
sessionInfo()
```
